# context vectors
import torch

inputs = torch.tensor(
    [[0.43, 0.15, 0.89],  # Your (x^1)
     [0.55, 0.87, 0.66],  # journey (x^2)
     [0.57, 0.85, 0.64],  # starts (x^3)
     [0.22, 0.58, 0.33],  # with (x^4)
     [0.77, 0.25, 0.10],  # one (x^5)
     [0.05, 0.80, 0.55]]  # step (x^6)
)

# ç¬¬ä¸€æ­¥éª¤æ˜¯è®¡ç®—ä¸­é—´å€¼wï¼Œ-> attention scoresï¼ˆcomputing the  attention scores ï· between the query x(2) and all other input elements as a dot product.ï¼‰
query = inputs[1]  # ç¬¬äºŒä¸ªè¾“å…¥tokenä½œä¸ºquery
shape0 = inputs.shape[0]
attn_scores_2 = torch.empty(shape0)
for i, x_i in enumerate(inputs):
    print(f"{x_i} dot {query} = {torch.dot(x_i, query)}")
    # We determine these scores by computing the dot
    # product of the query, x(2), with every other input token
    # ç‚¹ç§¯ç®—å‡ºæ¥æ˜¯ä¸€ä¸ªæ•°
    # ç‚¹ç§¯å€¼è¶Šé«˜ï¼Œä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„ç›¸ä¼¼åº¦å’Œæ³¨æ„åŠ›å¾—åˆ†è¶Šé«˜ã€‚
    # å‡ ä½•æ„ä¹‰
    #   ç‚¹ç§¯ç­‰äºä¸¤ä¸ªå‘é‡çš„æ¨¡é•¿ä¹˜ä»¥å®ƒä»¬ä¹‹é—´å¤¹è§’çš„ä½™å¼¦å€¼
    # aâ‹…b =âˆ¥aâˆ¥âˆ¥bâˆ¥cosÎ¸
    # ç‚¹ç§¯ä¸ºè´Ÿï¼Œè¡¨ç¤ºä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åã€‚
    attn_scores_2[i] = torch.dot(x_i, query)
print(attn_scores_2)
# å½’ä¸€åŒ–ï¼Œç›®çš„æ˜¯ä¿è¯æ³¨æ„åŠ›å¾—åˆ†çš„æ€»å’Œä¸º1
attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
print("Attention weights:", attn_weights_2_tmp)
print("Sum:", attn_weights_2_tmp.sum())


# é€šè¿‡softmaxå‡½æ•°ï¼Œå°†æ³¨æ„åŠ›å¾—åˆ†è½¬æ¢ä¸ºæ³¨æ„åŠ›æƒé‡
# å°†æ³¨æ„åŠ›åˆ†æ•°è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œçªå‡ºå…³é”®å…ƒç´ ï¼Œå‹ç¼©æ¬¡è¦å…ƒç´ ã€‚
# æä¾›æ•°å€¼ç¨³å®šæ€§å’Œè‰¯å¥½çš„æ¢¯åº¦ç‰¹æ€§ï¼Œæœ‰åŠ©äºè®­ç»ƒæ”¶æ•›ã€‚

# èƒ½ä¿è¯weight > 0
def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)


attn_weights_2_naive = softmax_naive(attn_scores_2)
print("Attention weights (naive softmax):", attn_weights_2_naive)
print("Sum:", attn_weights_2_naive.sum())

# ä½¿ç”¨PyTorchçš„å†…ç½®å‡½æ•°
attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights (PyTorch softmax):", attn_weights_2)
print("Sum:", attn_weights_2.sum())

# è®¡ç®—context vector
"""
é€šè¿‡å°†åµŒå…¥çš„è¾“å…¥ token ğ‘¥(ğ‘–)  ä¸å¯¹åº”çš„æ³¨æ„åŠ›æƒé‡ç›¸ä¹˜ï¼Œç„¶åå¯¹æ‰€å¾—çš„å‘é‡æ±‚å’Œï¼Œè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ğ‘§(2)ã€‚
å› æ­¤ï¼Œä¸Šä¸‹æ–‡å‘é‡ğ‘§(2)æ˜¯æ‰€æœ‰è¾“å…¥å‘é‡çš„åŠ æƒå’Œï¼Œå…·ä½“æ–¹æ³•æ˜¯å°†æ¯ä¸ªè¾“å…¥å‘é‡ä¸å…¶å¯¹åº”çš„æ³¨æ„åŠ›æƒé‡ç›¸ä¹˜ã€‚
"""

query = inputs[1]
context_vec_2 = torch.zeros(query.shape)
for i, x_i in enumerate(inputs):
    context_vec_2 += attn_weights_2[i] * x_i
print("Context vector:", context_vec_2)

## ------------------- è®¡ç®—æ‰€æœ‰çš„context vector -------------------

## step1: è®¡ç®—attention score
attn_scores = torch.empty(6, 6)
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)
# matrix multiplication
# attn_scores = inputs @ inputs.T

## step2: è®¡ç®—attention weights(å½’ä¸€åŒ–attention score)
attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)

## step3: è®¡ç®—context vector
all_context_vecs = attn_weights @ inputs
print(all_context_vecs)